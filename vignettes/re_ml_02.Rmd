---
title: 'Report Exercise 10: Supervised Machine Learning II'
author: 'Noémie Wellinger'
date: "`r Sys.Date()`"
output: 
 html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
---

# Introduction
This exercises further covers the process of supervised machine learning. Mainly, we look at the concept of the loss, and choosing right level of model complexity for optimal model generalisability as part of the model training step. 

We explore the role of structure in the data for model generalisability and how to best estimate a “true” out-of-sample error that corresponds to the prediction task. The task here is to train a model on ecosystem flux observations from one site and predict to another site (spatially upscaling). In previous examples and exercises, we always trained and predicted within the same site. How well is a model generalisable to a new site?

We investigate this question using ecosystem flux data from two distinct sites: Davos and Laegern. Again, use KNN and tune K for each model. Both sites are registered in the FLUXNET network.

# Methods 
## Setup and preprocessing
```{r setup, include=T, message=F}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(caret)
library(recipes)
library(tidyr)
library(yardstick)
library(zoo)
```

## Data Wrangling
Daily fluxes datasets from the FLUXNET network for the two sites Davos and Laegern are loaded and wrangled.
Because the timeseries of Davos (1997-2014) is longer than the one of Laegern (2004-2014), we crop the Davos dataset until 2004 as well to ensure comparability of the two models.
```{r wrangle_Dav, include=T, message=F}
daily_fluxes_Dav <- read_csv("../data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv") |>  
  
  # select only the variables we are interested in
  select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all all meteorological covariates
                -contains("JSB")   # weird useless variable
                ) |>

  # convert to a nice date object
  mutate(TIMESTAMP = ymd(TIMESTAMP)) |>

  # set all -9999 to NA
  mutate(across(where(is.numeric), ~na_if(., -9999))) |> 
  
  # retain only data based on >=80% good-quality measurements
  # overwrite bad data with NA (not dropping rows)
  mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF),
                TA_F           = ifelse(TA_F_QC        < 0.8, NA, TA_F),
                SW_IN_F        = ifelse(SW_IN_F_QC     < 0.8, NA, SW_IN_F),
                LW_IN_F        = ifelse(LW_IN_F_QC     < 0.8, NA, LW_IN_F),
                VPD_F          = ifelse(VPD_F_QC       < 0.8, NA, VPD_F),
                PA_F           = ifelse(PA_F_QC        < 0.8, NA, PA_F),
                P_F            = ifelse(P_F_QC         < 0.8, NA, P_F),
                WS_F           = ifelse(WS_F_QC        < 0.8, NA, WS_F)) |> 

  # drop QC variables (no longer needed)
  select(-ends_with("_QC")) |>
  
  # crop the years 1997-2003
  filter(year(TIMESTAMP) > 2003)
```



```{r wrangle_Lae, include=T, message=F}
daily_fluxes_Lae <- read_csv("../data/FLX_CH-Lae_FLUXNET2015_FULLSET_DD_2004-2014_1-4.csv") |>  
  
  # select only the variables we are interested in
  select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all all meteorological covariates
                -contains("JSB")   # weird useless variable
                ) |>

  # convert to a nice date object
  mutate(TIMESTAMP = ymd(TIMESTAMP)) |>

  # set all -9999 to NA
  mutate(across(where(is.numeric), ~na_if(., -9999))) |> 
  
  # retain only data based on >=80% good-quality measurements
  # overwrite bad data with NA (not dropping rows)
  mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF),
                TA_F           = ifelse(TA_F_QC        < 0.8, NA, TA_F),
                SW_IN_F        = ifelse(SW_IN_F_QC     < 0.8, NA, SW_IN_F),
                LW_IN_F        = ifelse(LW_IN_F_QC     < 0.8, NA, LW_IN_F),
                VPD_F          = ifelse(VPD_F_QC       < 0.8, NA, VPD_F),
                PA_F           = ifelse(PA_F_QC        < 0.8, NA, PA_F),
                P_F            = ifelse(P_F_QC         < 0.8, NA, P_F),
                WS_F           = ifelse(WS_F_QC        < 0.8, NA, WS_F)) |> 

  # drop QC variables (no longer needed)
  select(-ends_with("_QC"))
```

```{r vismiss, warning=F, message=F, fig.show="hold", out.width="50%", fig.cap="Figure 1a+b: Missing data in the Davos and Laegern datasets", }
visdat::vis_miss(
  daily_fluxes_Dav,
  cluster = FALSE, 
  warn_large_data = FALSE
  )
visdat::vis_miss(
  daily_fluxes_Lae,
  cluster = FALSE, 
  warn_large_data = FALSE
  )
```

We have to drop P_F in Laegern, since it is completely NA
```{r}
daily_fluxes_Lae <- daily_fluxes_Lae |>
  select(-P_F)
```

```{r hist_GPP, warning=F, message=F, fig.cap="Figure 2: Histogram of GPP observation values in Davos"}
# Data cleaning: looks ok, no obviously bad data
# no long tail, therefore no further target engineering
combined_data <- bind_rows(
  daily_fluxes_Dav %>% mutate(Location = "Dav"),
  daily_fluxes_Lae %>% mutate(Location = "Lae")
)

ggplot(combined_data, aes(x = GPP_NT_VUT_REF, fill = Location)) +
  geom_histogram(binwidth = 1, position = "identity", alpha = 0.5) +
  labs(x = "GPP_NT_VUT_REF",
       y = "Count") +
  scale_fill_manual(values = c("Dav" = "blue", "Lae" = "red"))
```


## Preprocessing
The function `split_train_test` is defined, which splits the FLUXNET dataset into training and test data.
80% of the data are used for training, 20% for testing for both sites for the true out-of-sample predictions.
Three model recipes are formulated.
```{r preprocess, message=F, warning=F}
set.seed(234) #for reproducibility
# load and run the data splitting function
source("../R/split_train_test.R")
split_Dav <- split_train_test(data=daily_fluxes_Dav, prop=0.8)
Dav_train <- split_Dav$train
Dav_test <- split_Dav$test

split_Lae <- split_train_test(data=daily_fluxes_Lae, prop=0.8)
Lae_train <- split_Lae$train
Lae_test <- split_Lae$test

#combine training data
pool_train <- bind_rows(Dav_train, Lae_train)

# Model and pre-processing formulation, use all variables but LW_IN_F
pp_Dav <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = Dav_train |> drop_na()) |> 
  recipes::step_BoxCox(all_predictors()) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())

# Model and pre-processing formulation, use all variables but LW_IN_F
pp_Lae <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = Lae_train |> drop_na()) |> 
  recipes::step_BoxCox(all_predictors()) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())

# Model and pre-processing formulation, use all variables but LW_IN_F
pp_pool <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = pool_train |> drop_na()) |> 
  recipes::step_BoxCox(all_predictors()) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())
```

# Results
## Within-site and across-site predictions
*Task*
Compare within-site predictions and across-site predictions on the test set for both sites, considering different metrics. For across-site predictions, make sure to implement a train and test setup that enables a true out-of-sample prediction test.

```{r eval_within_across, message=F, warning=F, fig.cap="Figure 3: Within and across-site predictions"}

source("../R/eval_plot_metrics.R")
    # predict Davos within
    plot_metrics_Dav_within <- eval_plot_metrics(pp_Dav, Dav_train, Dav_test, "Davos", type = "Within-site")

    plot_metrics_Lae_within <- eval_plot_metrics(pp_Lae, Lae_train, Lae_test, "Laegern", type = "Within-site")

    # predict Davos across with Laegern
    plot_metrics_Dav_across <- eval_plot_metrics(pp_Lae, Lae_train, Dav_test, "Davos", type = "Across-site")

    # predict Laegern across with Davos
    plot_metrics_Lae_across <- eval_plot_metrics(pp_Dav, Dav_train, Lae_test, "Laegern", type = "Across-site")
    
cowplot::plot_grid(plot_metrics_Dav_within$plot,
                   plot_metrics_Lae_within$plot,
                   plot_metrics_Dav_across$plot,
                   plot_metrics_Lae_across$plot)
```

## Predictions based on pooled training data
*Task*
Train a single model with training data pooled from both sites and predict with this single model on the test data of both sites. How do the model metrics on the test set compare to the true out-of-sample setup above? Interpret differences. Is it a valid approach to perform model training like this? Use your knowledge about structure in the data and its relevance for the model training setup.

```{r eval_pool, message=F, warning=F, fig.cap="Figure 4: KNN models trained on pooled training data from both sites"}
    # predict Davos with the pool training data
    plot_metrics_Dav_pool <- eval_plot_metrics(pp_pool, pool_train, Dav_test, "Davos", type = "Across-site, pool")
    # predict Laegern with the pool training data
    plot_metrics_Lae_pool <- eval_plot_metrics(pp_pool, pool_train, Lae_test, "Laegern", type = "Across-site, pool")
    
    cowplot::plot_grid(plot_metrics_Dav_pool$plot,
                       plot_metrics_Lae_pool$plot)
```

# Discussion

*Task*
Get information about the characteristics of the two sites. What are the differences in terms of climate, vegetation, altitude, etc. between the Davos and Laegern sites? Interpret biases of the out-of-sample predictions with a view to the site characteristics.

*Answers*
The models trained within-site (Fig. 3), have a better bias-variance trade-off than the ones trained across site. The models for Laegern also perform better in both cases The models trained on the pooled data (Fig. 4) have exactly the same metrics as the within-site (Davos) and the across-site (Laegern). I wasn't able to assess wether this is a coincidence or a consequence of the method or an error. 
Here, it should also be further assessed and clarified how the pooled training data should be created. Because in this exercises, I combined the two individual training sets of both sites together, the model had double the amount of training data available than the models making the true out-of-sample predictions.
The structure of the data given trough the two different sites obviously plays a role. As we saw in the histogram (Fig. 2), Laegern's observations of GPP are more dispersed over a slightly bigger range of values, while Davos' curve is narrower.
An overview of the most important site characteristics:

**CH-Dav: Davos**

* Elevation(m): 	1639
* Vegetation IGBP: 	ENF (Evergreen Needleleaf Forests: Lands dominated by woody vegetation with a percent cover >60% and height exceeding 2 meters. Almost all trees remain green all year. Canopy is never without green foliage.) 
* Climate Koeppen: 	ET (Tundra)
* Mean Annual Temp (°C): 	2.8
* Mean Annual Precip. (mm): 	1062

**CH-Lae: Laegern**

* Elevation(m): 	689
* Vegetation IGBP: 	MF (Mixed Forests: Lands dominated by trees with a percent cover >60% and height exceeding 2 meters. Consists of tree communities with interspersed mixtures or mosaics of the other four forest types. None of the forest types exceeds 60% of landscape.) 
* Climate Koeppen: 	—
* Mean Annual Temp (°C): 	8.3
* Mean Annual Precip. (mm): 	1100

Laegern, situated at lower altitude in a warmer climate and with mixed forests, will probably have a more pronounced growing season and achieve higher maximum GPP values than Davos, however Davos should have higher solar radiation through it's altitude and more GPP over the winter months.
Thus, it may very well be that the main drivers for GPP differ between these two sites. There are also direct spatial correlations between the two sites, with variable observations that are probably shifted, such as the start of the growing season, weather fronts coming from the west and hitting Laegern before Davos, etc. These shifts could be accounted for, when training the model in a combined way.
The higher MAE in the out-of-site prediction of Laegern could be because the model was not trained on such high values of GPP using the Davos training data. Regarding the R-squared of both across-site models, they perform almost equally in terms of variance, explaining around 50% of the model variance. 
